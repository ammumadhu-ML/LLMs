From OpenAI
- 1 token ~= 4 chars in English
- 1 token ~= Â¾ words
- 100 tokens ~= 75 words
- 1,500 words ~= 2k tokens
- Since, 500 words ~= 1 A4 page
- 4k ~= 6 A4 pages of text
- 5M Tokens is equal to entire GitHub repositories and thousands of documents.

1. https://thegradient.pub/in-context-learning-in-context/
2. http://ai.stanford.edu/blog/understanding-incontext/
3. [LARGER LANGUAGE MODELS DO IN-CONTEXT LEARNING DIFFERENTLY](https://arxiv.org/pdf/2303.03846.pdf)
4. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html
5. [Scaling Transformer to 1M tokens and beyond with RMT](https://arxiv.org/pdf/2304.11062.pdf) 
6. [Blockwise Parallel Transformer for Long Context Large Models](https://arxiv.org/pdf/2305.19370.pdf)
7. ["ğ—˜ğ˜…ğ˜ğ—²ğ—»ğ—±ğ—¶ğ—»ğ—´ ğ—–ğ—¼ğ—»ğ˜ğ—²ğ˜…ğ˜ ğ—¶ğ˜€ ğ—›ğ—®ğ—¿ğ—±â€¦ğ—¯ğ˜‚ğ˜ ğ—»ğ—¼ğ˜ ğ—œğ—ºğ—½ğ—¼ğ˜€ğ˜€ğ—¶ğ—¯ğ—¹ğ—²â€ explores and tests how to extend LLaMa to 8k](https://kaiokendev.github.io/context)
8. ["ğ—§ğ—µğ—² ğ—¦ğ—²ğ—°ğ—¿ğ—²ğ˜ ğ—¦ğ—®ğ˜‚ğ—°ğ—² ğ—¯ğ—²ğ—µğ—¶ğ—»ğ—± ğŸ­ğŸ¬ğŸ¬ğ— ğ—°ğ—¼ğ—»ğ˜ğ—²ğ˜…ğ˜ ğ˜„ğ—¶ğ—»ğ—±ğ—¼ğ˜„ ğ—¶ğ—» ğ—Ÿğ—Ÿğ— ğ˜€: ğ—®ğ—¹ğ—¹ ğ˜ğ—¿ğ—¶ğ—°ğ—¸ğ˜€ ğ—¶ğ—» ğ—¼ğ—»ğ—² ğ—½ğ—¹ğ—®ğ—°ğ—²â€ explores main paint points and tricks to extend the context length.](https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c)


Waitlist link: https://magic.dev/waitlist

![image](https://github.com/harirajeev/learn_LLMS/assets/13446418/0d7a98d0-31b1-4fd6-ab05-f04fcab97182)
