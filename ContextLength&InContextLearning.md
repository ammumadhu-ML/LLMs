From OpenAI
- 1 token ~= 4 chars in English
- 1 token ~= ¾ words
- 100 tokens ~= 75 words
- 1,500 words ~= 2k tokens
- Since, 500 words ~= 1 A4 page
- 4k ~= 6 A4 pages of text
- 5M Tokens is equal to entire GitHub repositories and thousands of documents.

1. https://thegradient.pub/in-context-learning-in-context/
2. http://ai.stanford.edu/blog/understanding-incontext/
3. [LARGER LANGUAGE MODELS DO IN-CONTEXT LEARNING DIFFERENTLY](https://arxiv.org/pdf/2303.03846.pdf)
4. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html
5. [Scaling Transformer to 1M tokens and beyond with RMT](https://arxiv.org/pdf/2304.11062.pdf) 
6. [Blockwise Parallel Transformer for Long Context Large Models](https://arxiv.org/pdf/2305.19370.pdf)
7. ["𝗘𝘅𝘁𝗲𝗻𝗱𝗶𝗻𝗴 𝗖𝗼𝗻𝘁𝗲𝘅𝘁 𝗶𝘀 𝗛𝗮𝗿𝗱…𝗯𝘂𝘁 𝗻𝗼𝘁 𝗜𝗺𝗽𝗼𝘀𝘀𝗶𝗯𝗹𝗲” explores and tests how to extend LLaMa to 8k](https://kaiokendev.github.io/context)
8. ["𝗧𝗵𝗲 𝗦𝗲𝗰𝗿𝗲𝘁 𝗦𝗮𝘂𝗰𝗲 𝗯𝗲𝗵𝗶𝗻𝗱 𝟭𝟬𝟬𝗞 𝗰𝗼𝗻𝘁𝗲𝘅𝘁 𝘄𝗶𝗻𝗱𝗼𝘄 𝗶𝗻 𝗟𝗟𝗠𝘀: 𝗮𝗹𝗹 𝘁𝗿𝗶𝗰𝗸𝘀 𝗶𝗻 𝗼𝗻𝗲 𝗽𝗹𝗮𝗰𝗲” explores main paint points and tricks to extend the context length.](https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c)


Waitlist link: https://magic.dev/waitlist

![image](https://github.com/harirajeev/learn_LLMS/assets/13446418/0d7a98d0-31b1-4fd6-ab05-f04fcab97182)
